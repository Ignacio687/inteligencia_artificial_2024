{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbzf5kkHZwa2Y7VvYwRqBj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Simple Linear Regression\n","\n","En este cuaderno veremos los conceptos detr谩s de una regresi贸n lineal simple: `Y = b0 + b1*X`\n","\n","* Intercepto `b0`\n","* Coeficiente `b1`\n","* Regresor `X`\n","* Objetivo `y`\n","\n","Tambi茅n veremos el concepto de OLS aplicado como medida de rendimiento para este algoritmo. Midiendo el error entre el valor de salida predicho por el modelo y el valor original para cada instancia de X, podremos mejorar el entrenamiento.\n","\n","La regresi贸n linear se basa en cuatro suposiciones:\n","1. Linearidad: significa que la variable dependiente tiene una relaci贸n lineal con la variable independiente.\n","2. Normalidad: significa que los errores de las observaciones estan normalmente distribuidos.\n","3. Independencia: significa que el error de cada observaci贸n es independiente de las dem谩s.\n","4. Homoscedasticidad: significa que la varianza del error de cada observaci贸n es constante para todas las observaciones.\n","5. Baja multi-colinearidad: significa que las variables independientes no estan altamente correlacionadas entre si.\n"],"metadata":{"id":"iisj3dqUSeri"}},{"cell_type":"markdown","source":["## 1. Creamos un set de datos\n","\n","En este caso a modo de ejemplo vamos a `crear` datos de 3 formas distintas:\n","\n","1. `np.arange`\n","2. `np.random.uniform`\n","3. `np.random.randn`\n","\n","La idea es que vean las diferencias entre las formas que posee NumPy. Para investigar cada m茅todo (ver que argumentos lleva y que retorna) pueden pararse dentro de los parentesis de cualquier funci贸n y usar la combinaci贸n `shift + ctrl + spacebar`"],"metadata":{"id":"qVZCCyz1SUN3"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"_RVUL9CmZGVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42) # Esta linea es para conservar los mismos valores \"random\" cada vez que se ejecute la celda\n","X = np.arange(0,300,dtype=int) #Empieza en 0 hasta el valor de stop que se elija, todos los valores son del tipo 'int'\n","X_uniform = np.random.uniform(0,1,size=300)\n","X_randn = np.random.randn(300)"],"metadata":{"id":"bxI8nB0BZIia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(X), type(X_uniform), type(X_randn)"],"metadata":{"id":"-jIm0Wf8iHRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"X values samples: {X[:5]}, value max: {X.max()}, value min: {X.min()}, mean: {X.mean()}, std: {X.std():.2f}\")\n","print(f\"X_uniform values samples: {X_uniform[:5]}, value max: {X_uniform.max()}, value min: {X_uniform.min()}, mean: {X_uniform.mean():.5f}, std: {X_uniform.std():.2f}\")\n","print(f\"X_randn values samples: {X_randn[:5]}, value max: {X_randn.max()}, value min:{X_randn.min()}, mean: {X_randn.mean():.5f}, std: {X_randn.std():.2f}\")"],"metadata":{"id":"gB77f1P4ftGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Visualizamos"],"metadata":{"id":"V5VWgkuLZ1rZ"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"CBKGCMJAaegs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizamos la distribuci贸n de los datos/valores de las variables independientes creadas\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,3,1)\n","plt.hist(X,density=True)\n","plt.title(\"Histograma de X\")\n","plt.subplot(1,3,2)\n","plt.hist(X_uniform,density=True)\n","plt.title(\"Histograma de X_uniform\")\n","plt.subplot(1,3,3)\n","plt.hist(X_randn,density=True)\n","plt.title(\"Histograma de X_randn\");"],"metadata":{"id":"p3ejSpDCah6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Armamos una relaci贸n lineal"],"metadata":{"id":"xAs2gUf0exor"}},{"cell_type":"code","source":["# Vamos a crear una relaci贸n entre una variable dependiente y las variables independientes creadas hasta ahora\n","# Usaremos para este ejemplo X y X_uniform\n","# La relaci贸n es lineal del tipo: y = X*coef + intercept\n","\n","coef = 0.5 #beta1\n","intercept = -1.3 #beta0\n","y = (X*coef) + intercept #En esta relaci贸n usamos como variable independiente el array X creado con np.arange()\n","y_uniform = (X_uniform*coef) + intercept #En esta relaci贸n usamos como variable independiente el array X_uniform creado con np.random.uniform()\n","y_randn = (X_randn*coef) + intercept"],"metadata":{"id":"avVNDGpr1xZm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(y), type(y_uniform), type(y_randn)"],"metadata":{"id":"6xsEf94Q3JmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Visualizamos"],"metadata":{"id":"4qa4GhE03NGd"}},{"cell_type":"code","source":["# Visualizamos la distribuci贸n de los datos/valores de las variables dependientes creadas\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,3,1)\n","plt.hist(X,density=True)\n","plt.title(\"Histograma de y\")\n","plt.subplot(1,3,2)\n","plt.hist(y_uniform,density=True)\n","plt.title(\"Histograma de y_uniform\")\n","plt.subplot(1,3,3)\n","plt.hist(y_randn,density=True)\n","plt.title(\"Histograma de y_randn\");"],"metadata":{"id":"o9d41Vl03W2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos ambas relaciones lineales\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,3,1)\n","plt.scatter(X,y,c='green',s=4)\n","plt.title(\"Linear X,y\")\n","plt.subplot(1,3,2)\n","plt.scatter(X_uniform,y_uniform,c='purple',s=4)\n","plt.title(\"Linear X_uniform,y_uniform\")\n","plt.subplot(1,3,3)\n","plt.scatter(X_randn,y_randn,c='lime',s=4)\n","plt.title(\"Linear X_randn,y_randn\");"],"metadata":{"id":"EfpbXOlR3xmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Armamos datos de entrenamiento y testeo\n","\n","Para avanzar en la pr谩ctica de regresi贸n lineal tomaremos el dataset de `X,y`.\n","Los pasos para implementar una regresi贸n son:\n","1. Preprocesar los datos (si es necesario normalizar o estandarizar).\n","2. Separar los datos en conjuntos de `train`, `test` y `validation`.\n","3. Elegimos el modelo a utilizar.\n","4. Entrenamos el modelo.\n","5. Evaluamos el modelo.\n","6. Ajustamos el modelo (en caso de ser necesario).\n","7. Repetimos el paso 5 y 6 hasta lograr m茅tricas que nos satisfagan.\n","8. Realizamos predicciones con datos nuevos."],"metadata":{"id":"AU0IznUr5h1h"}},{"cell_type":"code","source":["#Primero separamos de forma tradicional\n","split_size = int(len(X)*0.8)\n","print(f\"Tama帽o del set de entrenamiento: {split_size}\\nTama帽o del set de testeo: {len(X)-split_size}\")"],"metadata":{"id":"oJsQuNVpCMze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, y_train = X[:split_size], y[:split_size]\n","X_test, y_test = X[split_size:], y[split_size:]\n","print(f\"X_train length: {len(X_train)}, y_train length: {len(y_train)}\")\n","print(f\"X_test length: {len(X_test)}, y_test length: {len(y_test)}\")"],"metadata":{"id":"SspDtQ4p-1DD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,2,1)\n","plt.scatter(X,y,c='purple',s=2)\n","plt.title(\"Full dataset\")\n","plt.subplot(1,2,2)\n","plt.scatter(X_train,y_train,c='orange',s=4,label='Train set')\n","plt.scatter(X_test,y_test,c='green',s=4,label='Test set')\n","plt.title(\"Linear splited set\")\n","plt.legend();"],"metadata":{"id":"41-Dk9UjAH5T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">**Nota:** podemos ver que el set original de datos `X,y` ahora est谩 dividido en conjunto de entrenamiento `[X_train, y_train]` y de testeo `[X_test, y_test]` de forma **no aleatoria** lo cual en este caso el set de testeo es representativo de la forma de la funci贸n original `y = b0 + b1*w` pero en otros casos como una funci贸n `sigmoide` esto no funcionar铆a porque separar los datos de esta forma **seguramente** generar铆a un set de datos de testo **no representativos** del resto de datos."],"metadata":{"id":"b_i7M1gyBqNO"}},{"cell_type":"markdown","source":["### 5.1 Creamos un set de datos no lineales (SIGMOID)\n","\n"],"metadata":{"id":"IgDSOYRfC8oO"}},{"cell_type":"code","source":["# Creamos una relaci贸n con la funci贸n sigmoide implementada con numpy\n","x = np.linspace(-10, 10, 300)\n","z = 1/(1 + np.exp(-x))\n","print(f\"Tipo de estructura de datos: {type(z)}\")"],"metadata":{"id":"ITnbfmO5DFR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separamos el conjunto de datos X,z en train y test\n","X_train_sigm, y_train_sigm = x[:split_size], z[:split_size]\n","X_test_sigm, y_test_sigm = x[split_size:], z[split_size:]\n","print(f\"X_train length: {len(X_train_sigm)}, y_train length: {len(y_train_sigm)}\")\n","print(f\"X_test length: {len(X_test_sigm)}, y_test length: {len(y_test_sigm)}\")"],"metadata":{"id":"frHDwEqUGqRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo no lineales\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,2,1)\n","plt.scatter(x,z,c='purple',s=2)\n","plt.title(\"Full dataset Sigmoid\")\n","plt.subplot(1,2,2)\n","plt.scatter(X_train_sigm,y_train_sigm,c='orange',s=4,label='Train set')\n","plt.scatter(X_test_sigm,y_test_sigm,c='green',s=4,label='Test set')\n","plt.title(\"Sigmoid splited set\")\n","plt.legend();"],"metadata":{"id":"rxPJOtz9HERz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">**Nota:** Dado el ejemplo de arriba  vemos que hay que buscar una alternativa para separar los datos de forma aleatoria, as铆 poder conservar la forma de la relaci贸n en los datos de testeo."],"metadata":{"id":"zMvjT7e_KY-v"}},{"cell_type":"markdown","source":["## 6. Separamos datos de forma randomizada"],"metadata":{"id":"PA2LI5tmMu8r"}},{"cell_type":"code","source":["#para esto importamos scikit-learn que posee una funci贸n para separar los datos de forma ordenada\n","from sklearn.model_selection import train_test_split\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n","print(f\"Xtrain length: {len(Xtrain)}, ytrain length: {len(ytrain)}\")\n","print(f\"Xtest length: {len(Xtest)}, ytest length: {len(ytest)}\")"],"metadata":{"id":"cx5FeTYoBmDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo\n","plt.figure(figsize=(12,7))\n","plt.subplot(1,2,1)\n","plt.scatter(X,y,c='purple',s=2)\n","plt.title(\"Full dataset\")\n","plt.subplot(1,2,2)\n","plt.scatter(Xtrain,ytrain,c='orange',s=4,label='Train set')\n","plt.scatter(Xtest,ytest,c='green',s=6,label='Test set')\n","plt.title(\"Randomize splited set\")\n","plt.legend();"],"metadata":{"id":"Fs_O9DEPMpxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creamos una relaci贸n con la funci贸n sigmoide implementada con numpy\n","x_sigmoid = np.linspace(-10, 10, 300)\n","z = 1/(1 + np.exp(-x_sigmoid))\n","Xtrain_sigmoid, Xtest_sigmoid, ytrain_sigmoid, ytest_sigmoid = train_test_split(x_sigmoid, z, test_size=0.2, random_state=42)"],"metadata":{"id":"IoN7DBVzdglb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo no lineales\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,2,1)\n","plt.scatter(x_sigmoid,z,c='purple',s=2)\n","plt.title(\"Full dataset Sigmoid\")\n","plt.subplot(1,2,2)\n","# plt.scatter(Xtrain_sigmoid,ytrain_sigmoid,c='orange',s=4,label='Train set')\n","plt.scatter(Xtest_sigmoid,ytest_sigmoid,c='green',s=4,label='Test set')\n","plt.title(\"Sigmoid splited set\")\n","plt.legend();"],"metadata":{"id":"3Q3HLlLkdvkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Elegimos el modelo\n","\n","En este caso vamos a probar 3 modelos de regresi贸n, las tres estan basadas en el modelo OLS (m铆nimos cuadrados ordinarios).\n","\n","1. Linear Regression (OLS)\n","2. Lasso: basada en OLS pero incorpora un peso `Lambda` para castigar aquellos valores demasiado altos que nos pueden conducir a overfitting y un poco entendimiento de la verdadera relaci贸n entre las variables predictoras y el objetivo.\n","3. Ridge: basado en OLS y tambien incorpora una forma de calculo para castigar los valos muy altos y reducir notablemente el overfitting. Sin embargo, no realiza una selecci贸n de variables como es el caso de Lasso.\n","\n","El entrenamiento de los modelos lo haremos con el m茅todo `fit()` de cada objeto modelo instanciado. Usaremos el conjunto `[Xtrain,ytrain]` para entrenar y luego evaluaremos los modelos con el conjunto `[Xtest,ytest]`."],"metadata":{"id":"3xn-9CxXNGvU"}},{"cell_type":"code","source":["#Scikit-learn ya trae implementados muchos modelos, cuya hiper-parametrizaci贸n resulta sencilla de implementar\n","from sklearn.linear_model import  LinearRegression, Lasso, Ridge\n","linear_r = LinearRegression()\n","lasso_r = Lasso()\n","ridge_r = Ridge()"],"metadata":{"id":"hHXHYsyqNfZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtrain.reshape(-1,1).shape"],"metadata":{"id":"F2QKgRkSlKj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#entrenamos con la funci贸n .fit() y usamos el conjunto de entrenamiento solo para entrenar\n","linear_r.fit(Xtrain.reshape(-1,1),ytrain.reshape(-1,1))\n","lasso_r.fit(Xtrain.reshape(-1,1),ytrain.reshape(-1,1))\n","ridge_r.fit(Xtrain.reshape(-1,1),ytrain.reshape(-1,1))"],"metadata":{"id":"jr4e92X1OmBi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Evaluamos el modelo\n","\n","Para esto usamos el m茅todo `.score()` para calcular la metrica *coeficiente de determinaci贸n* o **R2** que incluye scikit-learn.\n","\n","Usaremos el conjunto de testeo `[Xtest,ytest]` para calcular este score."],"metadata":{"id":"O6KZ7lOxj5Qn"}},{"cell_type":"code","source":["#Primero evaluamos los modelos entrenados\n","print(f\"Linear Regression accuracy: {linear_r.score(Xtest.reshape(-1,1),ytest.reshape(-1,1))}\")\n","print(f\"Lasso Regression accuracy: {lasso_r.score(Xtest.reshape(-1,1),ytest.reshape(-1,1))}\")\n","print(f\"Ridge Regression accuracy: {ridge_r.score(Xtest.reshape(-1,1),ytest.reshape(-1,1))}\")"],"metadata":{"id":"CBKu6Du6mdMQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9. Hacemos predicciones\n","\n","Para hacer predicciones sobre el conjunto de testeo `[Xtest,ytest]`, ya que es el unico que no se uso durante el entrenamiento, ser铆a como poner a prueba el modelo con datos que nunca observ贸 hasta ahora.\n","\n","Tambi茅n calcularemos las siguientes m茅tricas:\n","1. R squared score.\n","2. Mean average error.\n","3. Mean squared error.\n","\n","Las metricas usadas son unas pocas de varias que ofrece scikit-learn https://scikit-learn.org/stable/modules/classes.html#regression-metrics.\n"],"metadata":{"id":"2C7IkbyFm4xP"}},{"cell_type":"code","source":["from sklearn import metrics as ms\n","def calculate_metrics(y_true:None,y_pred:None):\n","  \"\"\"Esta funcion calcula accuracy,mae,mse y retorna un diccionario con dichos valores\"\"\"\n","  r2 = ms.r2_score(y_true=y_true,y_pred=y_pred)\n","  mae = ms.mean_absolute_error(y_true=y_true,y_pred=y_pred)\n","  mse = ms.mean_squared_error(y_true=y_true,y_pred=y_pred)\n","  return {\"R2\":r2,\"MAE\":mae,\"MSE\":mse}"],"metadata":{"id":"6_CwDUCuoWfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Primero calculamos las metricas LinearRegression\n","y_pred = linear_r.predict(Xtest.reshape(-1,1))\n","linear_r_metrics = calculate_metrics(y_true=ytest.reshape(-1,1), y_pred=y_pred)\n","linear_r_metrics"],"metadata":{"id":"VJrWLOagqx3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Segundo calculamos las metricas Lasso\n","y_pred = lasso_r.predict(Xtest.reshape(-1,1))\n","lasso_r_metrics = calculate_metrics(y_true=ytest.reshape(-1,1), y_pred=y_pred)\n","lasso_r_metrics"],"metadata":{"id":"HVft8LGat6XJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tercero calculamos las metricas Ridge\n","y_pred = ridge_r.predict(Xtest.reshape(-1,1))\n","ridge_r_metrics = calculate_metrics(y_true=ytest.reshape(-1,1), y_pred=y_pred)\n","ridge_r_metrics"],"metadata":{"id":"LGzfhdlnvchC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","pd.DataFrame({\"Linear\":linear_r_metrics,\n","              \"Lasso\":lasso_r_metrics,\n","              \"Ridge\":ridge_r_metrics})"],"metadata":{"id":"BYtPETpJxRxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame({\"Linear\":linear_r_metrics,\n","              \"Lasso\":lasso_r_metrics,\n","              \"Ridge\":ridge_r_metrics}).T[['MSE']].plot(kind='bar');"],"metadata":{"id":"HzPvGF6HiwDm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10. Diagnostico del modelo"],"metadata":{"id":"_XxbpfE47r5y"}},{"cell_type":"code","source":["ytest.shape, y_pred.shape"],"metadata":{"id":"D0S-MXMAjQVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluamos el supuesto de linealidad de los residuos\n","from statsmodels.stats.stattools import durbin_watson\n","residuals = ytest - np.squeeze(y_pred)\n","dw = durbin_watson(residuals)\n","print(f\"Durbin Watson must be between [0,4] where both extremes are residual correlated and the middle 2 means independency between errors:\\n{dw}\")"],"metadata":{"id":"OBTzuNuH7rz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluamos el supuesto de homoscedasticidad\n","plt.scatter(y_pred,residuals)\n","plt.xlabel('Predicted')\n","plt.ylabel('Error')\n","plt.title('Homocedasticity');"],"metadata":{"id":"7uGX5Qnj8LXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluamos la normalidad\n","plt.hist(residuals)"],"metadata":{"id":"ZYSDQfH-8VTy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tambien podemos evaluar la linealidad de los residuos por quantiles (qqnorm)\n","import statsmodels.api as sm\n","import pylab\n","sm.qqplot(residuals,line='q')\n","pylab.show()"],"metadata":{"id":"_GgrdUjB8ZKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"yBYH7XEckleM"}},{"cell_type":"markdown","source":["> **Ejercicio**: crear un conjunto de datos para regression con `from sklearn.datasets import make_regression` con una sola feature, y si quieren le agrega noise (5)"],"metadata":{"id":"b20X93yDkNZL"}},{"cell_type":"markdown","source":["pd: Sigan los pasos del notebook"],"metadata":{"id":"1DXejBF9kkWa"}}]}