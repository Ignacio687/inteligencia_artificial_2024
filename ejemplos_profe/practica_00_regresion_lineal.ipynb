{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbzf5kkHZwa2Y7VvYwRqBj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Simple Linear Regression\n","\n","En este cuaderno veremos los conceptos detrás de una regresión lineal simple: `Y = b0 + b1*X`\n","\n","* Intercepto `b0`\n","* Coeficiente `b1`\n","* Regresor `X`\n","* Objetivo `y`\n","\n","También veremos el concepto de OLS aplicado como medida de rendimiento para este algoritmo. Midiendo el error entre el valor de salida predicho por el modelo y el valor original para cada instancia de X, podremos mejorar el entrenamiento.\n","\n","La regresión linear se basa en cuatro suposiciones:\n","1. Linearidad: significa que la variable dependiente tiene una relación lineal con la variable independiente.\n","2. Normalidad: significa que los errores de las observaciones estan normalmente distribuidos.\n","3. Independencia: significa que el error de cada observación es independiente de las demás.\n","4. Homoscedasticidad: significa que la varianza del error de cada observación es constante para todas las observaciones.\n","5. Baja multi-colinearidad: significa que las variables independientes no estan altamente correlacionadas entre si.\n"],"metadata":{"id":"iisj3dqUSeri"}},{"cell_type":"markdown","source":["## 1. Creamos un set de datos\n","\n","En este caso a modo de ejemplo vamos a `crear` datos de 3 formas distintas:\n","\n","1. `np.arange`\n","2. `np.random.uniform`\n","3. `np.random.randn`\n","\n","La idea es que vean las diferencias entre las formas que posee NumPy. Para investigar cada método (ver que argumentos lleva y que retorna) pueden pararse dentro de los parentesis de cualquier función y usar la combinación `shift + ctrl + spacebar`"],"metadata":{"id":"qVZCCyz1SUN3"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"_RVUL9CmZGVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42) # Esta linea es para conservar los mismos valores \"random\" cada vez que se ejecute la celda\n","X = np.arange(0,300,dtype=int) #Empieza en 0 hasta el valor de stop que se elija, todos los valores son del tipo 'int'\n","X_uniform = np.random.uniform(0,1,size=300)\n","X_randn = np.random.randn(300)"],"metadata":{"id":"bxI8nB0BZIia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(X), type(X_uniform), type(X_randn)"],"metadata":{"id":"-jIm0Wf8iHRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"X values samples: {X[:5]}, value max: {X.max()}, value min: {X.min()}, mean: {X.mean()}, std: {X.std():.2f}\")\n","print(f\"X_uniform values samples: {X_uniform[:5]}, value max: {X_uniform.max()}, value min: {X_uniform.min()}, mean: {X_uniform.mean():.5f}, std: {X_uniform.std():.2f}\")\n","print(f\"X_randn values samples: {X_randn[:5]}, value max: {X_randn.max()}, value min:{X_randn.min()}, mean: {X_randn.mean():.5f}, std: {X_randn.std():.2f}\")"],"metadata":{"id":"gB77f1P4ftGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Visualizamos"],"metadata":{"id":"V5VWgkuLZ1rZ"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"CBKGCMJAaegs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizamos la distribución de los datos/valores de las variables independientes creadas\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,3,1)\n","plt.hist(X,density=True)\n","plt.title(\"Histograma de X\")\n","plt.subplot(1,3,2)\n","plt.hist(X_uniform,density=True)\n","plt.title(\"Histograma de X_uniform\")\n","plt.subplot(1,3,3)\n","plt.hist(X_randn,density=True)\n","plt.title(\"Histograma de X_randn\");"],"metadata":{"id":"p3ejSpDCah6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Armamos una relación lineal"],"metadata":{"id":"xAs2gUf0exor"}},{"cell_type":"code","source":["# Vamos a crear una relación entre una variable dependiente y las variables independientes creadas hasta ahora\n","# Usaremos para este ejemplo X y X_uniform\n","# La relación es lineal del tipo: y = X*coef + intercept\n","\n","coef = 0.5 #beta1\n","intercept = -1.3 #beta0\n","y = (X*coef) + intercept #En esta relación usamos como variable independiente el array X creado con np.arange()\n","y_uniform = (X_uniform*coef) + intercept #En esta relación usamos como variable independiente el array X_uniform creado con np.random.uniform()\n","y_randn = (X_randn*coef) + intercept"],"metadata":{"id":"avVNDGpr1xZm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(y), type(y_uniform), type(y_randn)"],"metadata":{"id":"6xsEf94Q3JmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Visualizamos"],"metadata":{"id":"4qa4GhE03NGd"}},{"cell_type":"code","source":["# Visualizamos la distribución de los datos/valores de las variables dependientes creadas\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,3,1)\n","plt.hist(X,density=True)\n","plt.title(\"Histograma de y\")\n","plt.subplot(1,3,2)\n","plt.hist(y_uniform,density=True)\n","plt.title(\"Histograma de y_uniform\")\n","plt.subplot(1,3,3)\n","plt.hist(y_randn,density=True)\n","plt.title(\"Histograma de y_randn\");"],"metadata":{"id":"o9d41Vl03W2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos ambas relaciones lineales\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,3,1)\n","plt.scatter(X,y,c='green',s=4)\n","plt.title(\"Linear X,y\")\n","plt.subplot(1,3,2)\n","plt.scatter(X_uniform,y_uniform,c='purple',s=4)\n","plt.title(\"Linear X_uniform,y_uniform\")\n","plt.subplot(1,3,3)\n","plt.scatter(X_randn,y_randn,c='lime',s=4)\n","plt.title(\"Linear X_randn,y_randn\");"],"metadata":{"id":"EfpbXOlR3xmD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Armamos datos de entrenamiento y testeo\n","\n","Para avanzar en la práctica de regresión lineal tomaremos el dataset de `X,y`.\n","Los pasos para implementar una regresión son:\n","1. Preprocesar los datos (si es necesario normalizar o estandarizar).\n","2. Separar los datos en conjuntos de `train`, `test` y `validation`.\n","3. Elegimos el modelo a utilizar.\n","4. Entrenamos el modelo.\n","5. Evaluamos el modelo.\n","6. Ajustamos el modelo (en caso de ser necesario).\n","7. Repetimos el paso 5 y 6 hasta lograr métricas que nos satisfagan.\n","8. Realizamos predicciones con datos nuevos."],"metadata":{"id":"AU0IznUr5h1h"}},{"cell_type":"code","source":["#Primero separamos de forma tradicional\n","split_size = int(len(X)*0.8)\n","print(f\"Tamaño del set de entrenamiento: {split_size}\\nTamaño del set de testeo: {len(X)-split_size}\")"],"metadata":{"id":"oJsQuNVpCMze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, y_train = X[:split_size], y[:split_size]\n","X_test, y_test = X[split_size:], y[split_size:]\n","print(f\"X_train length: {len(X_train)}, y_train length: {len(y_train)}\")\n","print(f\"X_test length: {len(X_test)}, y_test length: {len(y_test)}\")"],"metadata":{"id":"SspDtQ4p-1DD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,2,1)\n","plt.scatter(X,y,c='purple',s=2)\n","plt.title(\"Full dataset\")\n","plt.subplot(1,2,2)\n","plt.scatter(X_train,y_train,c='orange',s=4,label='Train set')\n","plt.scatter(X_test,y_test,c='green',s=4,label='Test set')\n","plt.title(\"Linear splited set\")\n","plt.legend();"],"metadata":{"id":"41-Dk9UjAH5T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">**Nota:** podemos ver que el set original de datos `X,y` ahora está dividido en conjunto de entrenamiento `[X_train, y_train]` y de testeo `[X_test, y_test]` de forma **no aleatoria** lo cual en este caso el set de testeo es representativo de la forma de la función original `y = b0 + b1*w` pero en otros casos como una función `sigmoide` esto no funcionaría porque separar los datos de esta forma **seguramente** generaría un set de datos de testo **no representativos** del resto de datos."],"metadata":{"id":"b_i7M1gyBqNO"}},{"cell_type":"markdown","source":["### 5.1 Creamos un set de datos no lineales (SIGMOID)\n","\n"],"metadata":{"id":"IgDSOYRfC8oO"}},{"cell_type":"code","source":["# Creamos una relación con la función sigmoide implementada con numpy\n","x = np.linspace(-10, 10, 300)\n","z = 1/(1 + np.exp(-x))\n","print(f\"Tipo de estructura de datos: {type(z)}\")"],"metadata":{"id":"ITnbfmO5DFR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separamos el conjunto de datos X,z en train y test\n","X_train_sigm, y_train_sigm = x[:split_size], z[:split_size]\n","X_test_sigm, y_test_sigm = x[split_size:], z[split_size:]\n","print(f\"X_train length: {len(X_train_sigm)}, y_train length: {len(y_train_sigm)}\")\n","print(f\"X_test length: {len(X_test_sigm)}, y_test length: {len(y_test_sigm)}\")"],"metadata":{"id":"frHDwEqUGqRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo no lineales\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,2,1)\n","plt.scatter(x,z,c='purple',s=2)\n","plt.title(\"Full dataset Sigmoid\")\n","plt.subplot(1,2,2)\n","plt.scatter(X_train_sigm,y_train_sigm,c='orange',s=4,label='Train set')\n","plt.scatter(X_test_sigm,y_test_sigm,c='green',s=4,label='Test set')\n","plt.title(\"Sigmoid splited set\")\n","plt.legend();"],"metadata":{"id":"rxPJOtz9HERz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[">**Nota:** Dado el ejemplo de arriba 👆 vemos que hay que buscar una alternativa para separar los datos de forma aleatoria, así poder conservar la forma de la relación en los datos de testeo."],"metadata":{"id":"zMvjT7e_KY-v"}},{"cell_type":"markdown","source":["## 6. Separamos datos de forma randomizada"],"metadata":{"id":"PA2LI5tmMu8r"}},{"cell_type":"code","source":["#para esto importamos scikit-learn que posee una función para separar los datos de forma ordenada\n","from sklearn.model_selection import train_test_split\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n","print(f\"Xtrain length: {len(Xtrain)}, ytrain length: {len(ytrain)}\")\n","print(f\"Xtest length: {len(Xtest)}, ytest length: {len(ytest)}\")"],"metadata":{"id":"cx5FeTYoBmDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo\n","plt.figure(figsize=(12,7))\n","plt.subplot(1,2,1)\n","plt.scatter(X,y,c='purple',s=2)\n","plt.title(\"Full dataset\")\n","plt.subplot(1,2,2)\n","plt.scatter(Xtrain,ytrain,c='orange',s=4,label='Train set')\n","plt.scatter(Xtest,ytest,c='green',s=6,label='Test set')\n","plt.title(\"Randomize splited set\")\n","plt.legend();"],"metadata":{"id":"Fs_O9DEPMpxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creamos una relación con la función sigmoide implementada con numpy\n","x_sigmoid = np.linspace(-10, 10, 300)\n","z = 1/(1 + np.exp(-x_sigmoid))\n","Xtrain_sigmoid, Xtest_sigmoid, ytrain_sigmoid, ytest_sigmoid = train_test_split(x_sigmoid, z, test_size=0.2, random_state=42)"],"metadata":{"id":"IoN7DBVzdglb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Visualizamos los datos de entrenamiento y testeo no lineales\n","plt.figure(figsize=(10,7))\n","plt.subplot(1,2,1)\n","plt.scatter(x_sigmoid,z,c='purple',s=2)\n","plt.title(\"Full dataset Sigmoid\")\n","plt.subplot(1,2,2)\n","# plt.scatter(Xtrain_sigmoid,ytrain_sigmoid,c='orange',s=4,label='Train set')\n","plt.scatter(Xtest_sigmoid,ytest_sigmoid,c='green',s=4,label='Test set')\n","plt.title(\"Sigmoid splited set\")\n","plt.legend();"],"metadata":{"id":"3Q3HLlLkdvkQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Elegimos el modelo\n","\n","En este caso vamos a probar 3 modelos de regresión, las tres estan basadas en el modelo OLS (mínimos cuadrados ordinarios).\n","\n","1. Linear Regression (OLS)\n","2. Lasso: basada en OLS pero incorpora un peso `Lambda` para castigar aquellos valores demasiado altos que nos pueden conducir a overfitting y un poco entendimiento de la verdadera relación entre las variables predictoras y el objetivo.\n","3. Ridge: basado en OLS y tambien incorpora una forma de calculo para castigar los valos muy altos y reducir notablemente el overfitting. Sin embargo, no realiza una selección de variables como es el caso de Lasso.\n","\n","El entrenamiento de los modelos lo haremos con el método `fit()` de cada objeto modelo instanciado. Usaremos el conjunto `[Xtrain,ytrain]` para entrenar y luego evaluaremos los modelos con el conjunto `[Xtest,ytest]`."],"metadata":{"id":"3xn-9CxXNGvU"}},{"cell_type":"code","source":["#Scikit-learn ya trae implementados muchos modelos, cuya hiper-parametrización resulta sencilla de implementar\n","from sklearn.linear_model import  LinearRegression, Lasso, Ridge\n","linear_r = LinearRegression()\n","lasso_r = Lasso()\n","ridge_r = Ridge()"],"metadata":{"id":"hHXHYsyqNfZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtrain.reshape(-1,1).shape"],"metadata":{"id":"F2QKgRkSlKj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#entrenamos con la función .fit() y usamos el conjunto de entrenamiento solo para entrenar\n","linear_r.fit(Xtrain.reshape(-1,1),ytrain.reshape(-1,1))\n","lasso_r.fit(Xtrain.reshape(-1,1),ytrain.reshape(-1,1))\n","ridge_r.fit(Xtrain.reshape(-1,1),ytrain.reshape(-1,1))"],"metadata":{"id":"jr4e92X1OmBi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Evaluamos el modelo\n","\n","Para esto usamos el método `.score()` para calcular la metrica *coeficiente de determinación* o **R2** que incluye scikit-learn.\n","\n","Usaremos el conjunto de testeo `[Xtest,ytest]` para calcular este score."],"metadata":{"id":"O6KZ7lOxj5Qn"}},{"cell_type":"code","source":["#Primero evaluamos los modelos entrenados\n","print(f\"Linear Regression accuracy: {linear_r.score(Xtest.reshape(-1,1),ytest.reshape(-1,1))}\")\n","print(f\"Lasso Regression accuracy: {lasso_r.score(Xtest.reshape(-1,1),ytest.reshape(-1,1))}\")\n","print(f\"Ridge Regression accuracy: {ridge_r.score(Xtest.reshape(-1,1),ytest.reshape(-1,1))}\")"],"metadata":{"id":"CBKu6Du6mdMQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9. Hacemos predicciones\n","\n","Para hacer predicciones sobre el conjunto de testeo `[Xtest,ytest]`, ya que es el unico que no se uso durante el entrenamiento, sería como poner a prueba el modelo con datos que nunca observó hasta ahora.\n","\n","También calcularemos las siguientes métricas:\n","1. R squared score.\n","2. Mean average error.\n","3. Mean squared error.\n","\n","Las metricas usadas son unas pocas de varias que ofrece scikit-learn https://scikit-learn.org/stable/modules/classes.html#regression-metrics.\n"],"metadata":{"id":"2C7IkbyFm4xP"}},{"cell_type":"code","source":["from sklearn import metrics as ms\n","def calculate_metrics(y_true:None,y_pred:None):\n","  \"\"\"Esta funcion calcula accuracy,mae,mse y retorna un diccionario con dichos valores\"\"\"\n","  r2 = ms.r2_score(y_true=y_true,y_pred=y_pred)\n","  mae = ms.mean_absolute_error(y_true=y_true,y_pred=y_pred)\n","  mse = ms.mean_squared_error(y_true=y_true,y_pred=y_pred)\n","  return {\"R2\":r2,\"MAE\":mae,\"MSE\":mse}"],"metadata":{"id":"6_CwDUCuoWfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Primero calculamos las metricas LinearRegression\n","y_pred = linear_r.predict(Xtest.reshape(-1,1))\n","linear_r_metrics = calculate_metrics(y_true=ytest.reshape(-1,1), y_pred=y_pred)\n","linear_r_metrics"],"metadata":{"id":"VJrWLOagqx3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Segundo calculamos las metricas Lasso\n","y_pred = lasso_r.predict(Xtest.reshape(-1,1))\n","lasso_r_metrics = calculate_metrics(y_true=ytest.reshape(-1,1), y_pred=y_pred)\n","lasso_r_metrics"],"metadata":{"id":"HVft8LGat6XJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tercero calculamos las metricas Ridge\n","y_pred = ridge_r.predict(Xtest.reshape(-1,1))\n","ridge_r_metrics = calculate_metrics(y_true=ytest.reshape(-1,1), y_pred=y_pred)\n","ridge_r_metrics"],"metadata":{"id":"LGzfhdlnvchC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","pd.DataFrame({\"Linear\":linear_r_metrics,\n","              \"Lasso\":lasso_r_metrics,\n","              \"Ridge\":ridge_r_metrics})"],"metadata":{"id":"BYtPETpJxRxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame({\"Linear\":linear_r_metrics,\n","              \"Lasso\":lasso_r_metrics,\n","              \"Ridge\":ridge_r_metrics}).T[['MSE']].plot(kind='bar');"],"metadata":{"id":"HzPvGF6HiwDm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10. Diagnostico del modelo"],"metadata":{"id":"_XxbpfE47r5y"}},{"cell_type":"code","source":["ytest.shape, y_pred.shape"],"metadata":{"id":"D0S-MXMAjQVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluamos el supuesto de linealidad de los residuos\n","from statsmodels.stats.stattools import durbin_watson\n","residuals = ytest - np.squeeze(y_pred)\n","dw = durbin_watson(residuals)\n","print(f\"Durbin Watson must be between [0,4] where both extremes are residual correlated and the middle 2 means independency between errors:\\n{dw}\")"],"metadata":{"id":"OBTzuNuH7rz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluamos el supuesto de homoscedasticidad\n","plt.scatter(y_pred,residuals)\n","plt.xlabel('Predicted')\n","plt.ylabel('Error')\n","plt.title('Homocedasticity');"],"metadata":{"id":"7uGX5Qnj8LXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluamos la normalidad\n","plt.hist(residuals)"],"metadata":{"id":"ZYSDQfH-8VTy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Tambien podemos evaluar la linealidad de los residuos por quantiles (qqnorm)\n","import statsmodels.api as sm\n","import pylab\n","sm.qqplot(residuals,line='q')\n","pylab.show()"],"metadata":{"id":"_GgrdUjB8ZKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"yBYH7XEckleM"}},{"cell_type":"markdown","source":[">🛠 **Ejercicio**: crear un conjunto de datos para regression con `from sklearn.datasets import make_regression` con una sola feature, y si quieren le agrega noise (5)"],"metadata":{"id":"b20X93yDkNZL"}},{"cell_type":"markdown","source":["pd: Sigan los pasos del notebook"],"metadata":{"id":"1DXejBF9kkWa"}}]}